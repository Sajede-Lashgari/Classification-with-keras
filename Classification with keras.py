# -*- coding: utf-8 -*-
"""lastV_iris.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ikhs0uu1R2_g-WmNsNrPqp5eW9qB1Del
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

#libraries

import numpy as np
from sklearn import datasets
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import warnings
from keras.models import Sequential
from keras.layers import Dense
from sklearn.preprocessing import LabelBinarizer
from keras.wrappers.scikit_learn import KerasClassifier
from matplotlib import pyplot as plt
from keras import optimizers
from keras.callbacks import TensorBoard
from sklearn.model_selection import cross_val_score
from numpy.random import seed
from tensorflow import set_random_seed
seed(1)
seed=2
set_random_seed(2)

# load data and preprocessing

def rhead(x, nrow = 6, ncol = 4):
    pd.set_option('display.expand_frame_repr', False)
    seq = np.arange(0, len(x.columns), ncol)
    for i in seq:
        print(x.loc[range(0, nrow), x.columns[
                range(i, min(i+ncol, len(x.columns)))]])
    pd.set_option('display.expand_frame_repr', True)
    
encoder = LabelBinarizer()
iris = datasets.load_iris()
iris_data_df = pd.DataFrame(data=iris.data, columns=iris.feature_names,
                       dtype=np.float32)
target = encoder.fit_transform(iris.target)
iris_target_df = pd.DataFrame(data=target, columns=iris.target_names)

X_train, X_test, y_train, y_test = train_test_split(iris_data_df,
                                                 iris_target_df,
                                                 test_size=0.2,
                                                 random_state=seed)
scaler = MinMaxScaler(feature_range=(0,1))

X_train = pd.DataFrame(scaler.fit_transform(X_train),
                               columns=X_train.columns,
                               index=X_train.index)
X_test = pd.DataFrame(scaler.transform(X_test),
                           columns=X_test.columns,
                           index=X_test.index)

# soale 1.A
def model():
    """build the Keras model callback"""
    model = Sequential()
    model.add(Dense(3, input_dim=4, activation='elu', name='layer_1'))
    model.add(Dense(3, activation='elu', name='layer_2'))
    model.add(Dense(3, activation='elu', name='layer_3'))
    model.add(Dense(3, activation='softmax', name='output_layer'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

model().summary()

model = model()
history = model.fit(
       X_train,
       y_train,
      epochs=200, batch_size=1, validation_split=0.15,
       shuffle=True,
       verbose=2 # this will tell keras to print more detailed info
       # during trainnig to know what is going on
       )

#run the test dataset
test_error_rate = model.evaluate(X_test, y_test, verbose=0)
print(
      "{} : {:.2f}%".format(model.metrics_names[1],
              test_error_rate[1]*100))
print(
      "{} : {:.2f}%".format(model.metrics_names[0],
              test_error_rate[0]*100))

#run the train dataset
train_error_rate = model.evaluate(X_train, y_train, verbose=2)
print(
      "{} : {:.2f}%".format(model.metrics_names[1],
              train_error_rate[1]*100))
print(
      "{} : {:.2f}%".format(model.metrics_names[0],
              train_error_rate[0]*100))

print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

# soale 1.B

warnings.simplefilter(action='ignore', category=FutureWarning)

n_features = X_train.shape[1]
n_classes = y_train.shape[1]

def create_custom_model(input_dim, output_dim, nodes, n=1, name='model'):
    def create_model():
        # Create model
        model = Sequential(name=name)
        for i in range(n):
            model.add(Dense(nodes, input_dim=input_dim, activation='elu'))
        model.add(Dense(output_dim, activation='softmax'))
        # Compile model
        model.compile(loss='categorical_crossentropy', 
                      optimizer='adam', 
                      metrics=['accuracy'])
        return model
    return create_model

models = [create_custom_model(n_features, n_classes, 3, i, 'model_{}'.format(i)) 
          for i in range(1, 11)]

for create_model in models:
    create_model().summary()

history_dict = {}

# TensorBoard Callback
cb = TensorBoard()

for create_model in models:
    model = create_model()
    print('Model name:', model.name)
    history_callback = model.fit(X_train, y_train,
                                 batch_size=8,
                                 epochs=100,
                                 verbose=2,
                                 validation_data=(X_test, y_test),
                                 callbacks=[cb])
        
    history_dict[model.name] = [history_callback, model]

fig, (ax1, ax2) = plt.subplots(2, figsize=(15, 17))
plt.figure()

for model_name in history_dict: 
    val_acc = history_dict[model_name][0].history['val_acc']
    val_loss = history_dict[model_name][0].history['val_loss']
    ax1.plot(val_acc, label=model_name)
    ax2.plot(val_loss, label=model_name)
    
ax1.set_ylabel('test accuracy')
ax2.set_ylabel('test loss')
ax2.set_xlabel('epochs')
ax1.legend()
ax2.legend()
plt.show()

fig, (ax1, ax2) = plt.subplots(2, figsize=(15, 17))

for model_name in history_dict:
    val_acc = history_dict[model_name][0].history['acc']
    val_loss = history_dict[model_name][0].history['loss']
    ax1.plot(val_acc, label=model_name)
    ax2.plot(val_loss, label=model_name)
    
ax1.set_ylabel('training accuracy')
ax2.set_ylabel('training loss')
ax2.set_xlabel('epochs')
ax1.legend()
ax2.legend()
plt.show()

for create_model in models:
    model = create_model()
    print('Model name:', model.name)
    scoreS = model.evaluate(X_test, y_test, verbose=0)
    print('Test loss:', scoreS[0])
    print('Test accuracy:', scoreS[1])
    scoreR = model.evaluate(X_train, y_train, verbose=0)
    print('Train loss:', scoreR[0])
    print('Train accuracy:', scoreR[1])

N=[]
AT=[]
AS=[]
LS=[]
LT=[]
for create_model in models:
    model = create_model()
    N.append(model.name)
    scoreS = model.evaluate(X_test, y_test, verbose=0)
    LS.append(scoreS[0])
    AS.append(scoreS[1])
    scoreR = model.evaluate(X_train, y_train, verbose=0)
    LT.append(scoreR[0])
    AT.append(scoreR[1])


plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
plt.plot(N,AS, label='Test accuray')
plt.plot(N,AT, label='Train accuray')
plt.xlabel("Number of hidden layer")
plt.ylabel('Accuracy')
plt.legend();

plt.subplot(1, 2, 2)
plt.plot(N,LS, label='Test loss')
plt.plot(N,LT, label='Train loss')
plt.xlabel("Number of hidden layer")
plt.ylabel('Loss')
plt.legend();

X = iris['data']
y = iris['target']
names = iris['target_names']
feature_names = iris['feature_names']

# Visualize the data sets
plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
for target, target_name in enumerate(names):
    X_plot = X[y == target]
    plt.plot(X_plot[:, 0], X_plot[:, 1], linestyle='none', marker='o', label=target_name)
plt.xlabel(feature_names[0])
plt.ylabel(feature_names[1])
plt.axis('equal')
plt.legend();

plt.subplot(1, 2, 2)
for target, target_name in enumerate(names):
    X_plot = X[y == target]
    plt.plot(X_plot[:, 2], X_plot[:, 3], linestyle='none', marker='o', label=target_name)
plt.xlabel(feature_names[2])
plt.ylabel(feature_names[3])
plt.axis('equal')
plt.legend();

#soale 2

def model():
    """build the Keras model callback"""
    model = Sequential()
    model.add(Dense(3, input_dim=4, activation='softmax', name='layer_1'))
    model.add(Dense(3, activation='softmax', name='layer_2'))
    model.add(Dense(3, activation='softmax', name='layer_3'))
    model.add(Dense(3, activation='softmax', name='output_layer'))
    sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True) #Nesterov
    model.compile(loss='MSE', optimizer=sgd, metrics=['accuracy']) #MSE
    #sgd = optimizers.SGD(lr=0.01, momentum=0, nesterov=False) #SGD
    #sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False) #momentum
    #model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) #categorical_crossentropy
    return model

model = model()
history = model.fit(
       X_train,
       y_train,
      epochs=500, batch_size=1
      #len(X_train) for gradient descent
      ,validation_split=0.15,shuffle=True,
      verbose=2)

#run the test dataset
test_error_rate = model.evaluate(X_test, y_test, verbose=0)
print(
      "{} : {:.2f}%".format(model.metrics_names[1],
              test_error_rate[1]*100))
print(
      "{} : {:.2f}%".format(model.metrics_names[0],
              test_error_rate[0]*100))

#run the train dataset
train_error_rate = model.evaluate(X_train, y_train, verbose=2)
print(
      "{} : {:.2f}%".format(model.metrics_names[1],
              train_error_rate[1]*100))
print(
      "{} : {:.2f}%".format(model.metrics_names[0],
              train_error_rate[0]*100))

print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

